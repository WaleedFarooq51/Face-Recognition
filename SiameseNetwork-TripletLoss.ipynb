{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FLdynJXX7CUf"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iM3cmZA0zIxE"},"outputs":[],"source":["%cd /content/drive/My Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hho3Dl4sgYBy"},"outputs":[],"source":["! 7z x dataset.7z -aoa -o/content/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IylgR_8q6VkM"},"outputs":[],"source":["import os\n","import cv2\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","import tensorflow as tf\n","from datetime import datetime\n","from sklearn.utils import shuffle\n","import pprint\n","\n","%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0r_TP4fk7GsO"},"outputs":[],"source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DjbSL-r6Vkq"},"outputs":[],"source":["epochs = 50\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.00006)\n","binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\n","\n","base_dir = \"\"\n","\n","stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","logdir = os.path.join(base_dir, 'logs/func/%s' % stamp)\n","writer = tf.summary.create_file_writer(logdir)\n","\n","scalar_logdir = os.path.join(base_dir, 'logs/scalars/%s' % stamp)\n","file_writer = tf.summary.create_file_writer(scalar_logdir + \"/metrics\")\n","\n","checkpoint_path = os.path.join(base_dir, 'logs/model/siamese')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wdihg3_oRzrH"},"outputs":[],"source":["'''\n","The following code cell is taken from the source code of keras_vggface.\n","I tried using the preprocess_input function provided by tf.keras but they provide different results.\n","To my knowledge, it seems that the mean values which are subtracted in each image are different.\n","'''\n","K = tf.keras.backend\n","\n","def preprocess_input(x, data_format=None, version=1):\n","    x_temp = np.copy(x)\n","    if data_format is None:\n","        data_format = K.image_data_format()\n","    assert data_format in {'channels_last', 'channels_first'}\n","\n","    if version == 1:\n","        if data_format == 'channels_first':\n","            x_temp = x_temp[:, ::-1, ...]\n","            x_temp[:, 0, :, :] -= 93.5940\n","            x_temp[:, 1, :, :] -= 104.7624\n","            x_temp[:, 2, :, :] -= 129.1863\n","        else:\n","            x_temp = x_temp[..., ::-1]\n","            x_temp[..., 0] -= 93.5940\n","            x_temp[..., 1] -= 104.7624\n","            x_temp[..., 2] -= 129.1863\n","\n","    elif version == 2:\n","        if data_format == 'channels_first':\n","            x_temp = x_temp[:, ::-1, ...]\n","            x_temp[:, 0, :, :] -= 91.4953\n","            x_temp[:, 1, :, :] -= 103.8827\n","            x_temp[:, 2, :, :] -= 131.0912\n","        else:\n","            x_temp = x_temp[..., ::-1]\n","            x_temp[..., 0] -= 91.4953\n","            x_temp[..., 1] -= 103.8827\n","            x_temp[..., 2] -= 131.0912\n","    else:\n","        raise NotImplementedError\n","\n","    return x_temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGiVwj9HRzrM"},"outputs":[],"source":["class DataGenerator(tf.keras.utils.Sequence):\n","    def __init__(self, dataset_path, batch_size=30, shuffle=True):\n","        self.dataset = self.curate_dataset(dataset_path)\n","        self.dataset_path = dataset_path\n","        self.shuffle = shuffle\n","        self.batch_size =batch_size\n","        self.no_of_people = len(list(self.dataset.keys()))\n","        self.on_epoch_end()\n","\n","    def __getitem__(self, index):\n","        people = list(self.dataset.keys())[index * self.batch_size: (index + 1) * self.batch_size]\n","        P = []\n","        A = []\n","        N = []\n","\n","        for person in people:\n","            if not self.dataset[person] or len(self.dataset[person]) == 1:\n","              continue\n","\n","            anchor_index = random.randint(0, len(self.dataset[person])-1)\n","            a = self.get_image(person, anchor_index)\n","\n","            positive_index = random.randint(0, len(self.dataset[person])-1)\n","            while positive_index == anchor_index:\n","                positive_index = random.randint(0, len(self.dataset[person])-1)\n","            p = self.get_image(person, positive_index)\n","\n","            negative_person_index = random.randint(0, self.no_of_people - 1)\n","            negative_person = list(self.dataset.keys())[negative_person_index]\n","            while negative_person == person:\n","                negative_person_index = random.randint(0, self.no_of_people - 1)\n","                negative_person = list(self.dataset.keys())[negative_person_index]\n","\n","            negative_index = random.randint(0, len(self.dataset[negative_person])-1)\n","            n = self.get_image(negative_person, negative_index)\n","            P.append(p)\n","            A.append(a)\n","            N.append(n)\n","        A = np.asarray(A)\n","        N = np.asarray(N)\n","        P = np.asarray(P)\n","        #print(\"succesfully create triplets\")\n","        return [A, P, N]\n","\n","\n","    def __len__(self):\n","        return self.no_of_people // self.batch_size\n","\n","    def curate_dataset(self, dataset_path):\n","        with open(os.path.join(dataset_path, 'list.txt'), 'r') as f:\n","            dataset = {}\n","            image_list = f.read().split()\n","            print(image_list)\n","            for image in image_list:\n","                try:\n","                  folder_name, file_name = image.split('/')\n","                except ValueError:\n","                  print(f\"Error splitting image path: {image}\")\n","                  continue\n","                if folder_name in dataset.keys():\n","                    dataset[folder_name].append(file_name)\n","                else:\n","                    dataset[folder_name] = [file_name]\n","\n","        pprint.pprint(dataset)\n","        print(dataset.keys())\n","        print(len(dataset))\n","        return dataset\n","\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            keys = list(self.dataset.keys())\n","            random.shuffle(keys)\n","            dataset_ =  {}\n","            for key in keys:\n","                dataset_[key] = self.dataset[key]\n","            self.dataset = dataset_\n","\n","    def get_image(self, person, index):\n","        img = cv2.imread(os.path.join(self.dataset_path, os.path.join('images/' + person, self.dataset[person][index])))\n","        img = cv2.resize(img, (224, 224))\n","        img = np.asarray(img, dtype=np.float64)\n","        img = preprocess_input(img)\n","        return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIKi79ULRzrW"},"outputs":[],"source":["vggface = tf.keras.models.Sequential()\n","vggface.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding=\"SAME\", input_shape=(224,224, 3)))\n","vggface.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Flatten())\n","\n","vggface.add(tf.keras.layers.Dense(4096, activation='relu'))\n","vggface.add(tf.keras.layers.Dropout(0.5))\n","vggface.add(tf.keras.layers.Dense(4096, activation='relu'))\n","vggface.add(tf.keras.layers.Dropout(0.5))\n","vggface.add(tf.keras.layers.Dense(2622, activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"laWuFpaGRzrc"},"outputs":[],"source":["vggface.load_weights(os.path.join(base_dir, 'vgg_face_weights.h5'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afKW7aTnRzrh"},"outputs":[],"source":["vggface.pop()\n","vggface.add(tf.keras.layers.Dense(128, use_bias=False))\n","\n","for layer in vggface.layers[:-2]:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aolP7nPcRzrn"},"outputs":[],"source":["class SiameseNetwork(tf.keras.Model):\n","    def __init__(self, vgg_face):\n","        super(SiameseNetwork, self).__init__()\n","        self.vgg_face = vgg_face\n","\n","    @tf.function\n","    def call(self, inputs):\n","        image_1, image_2, image_3 =  inputs\n","        with tf.name_scope(\"Anchor\") as scope:\n","            feature_1 = self.vgg_face(image_1)\n","            feature_1 = tf.math.l2_normalize(feature_1, axis=-1)\n","        with tf.name_scope(\"Positive\") as scope:\n","            feature_2 = self.vgg_face(image_2)\n","            feature_2 = tf.math.l2_normalize(feature_2, axis=-1)\n","        with tf.name_scope(\"Negative\") as scope:\n","            feature_3 = self.vgg_face(image_3)\n","            feature_3 = tf.math.l2_normalize(feature_3, axis=-1)\n","        return [feature_1, feature_2, feature_3]\n","\n","    @tf.function\n","    def get_features(self, inputs):\n","        return tf.math.l2_normalize(self.vgg_face(inputs), axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_fwdmOvRzrt"},"outputs":[],"source":["model = SiameseNetwork(vggface)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdsZ6V-fRzrx"},"outputs":[],"source":["def loss_function(x, alpha = 0.2):\n","    # Triplet Loss function.\n","    anchor,positive,negative = x\n","    # distance between the anchor and the positive\n","    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n","    # distance between the anchor and the negative\n","    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n","    # compute loss\n","    basic_loss = pos_dist-neg_dist+alpha\n","    loss = K.mean(K.maximum(basic_loss,0.0))\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLjMBa96Rzr3"},"outputs":[],"source":["def train(X):\n","    with tf.GradientTape() as tape:\n","        y_pred = model(X)\n","        loss = loss_function(y_pred)\n","    grad = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pg1F6xngRzr7"},"outputs":[],"source":["tf.summary.trace_on(graph=True, profiler=False)\n","output = model([tf.zeros((32,224,224,3)), tf.zeros((32,224,224,3)), tf.zeros((32,224,224,3))])\n","with writer.as_default():\n","    tf.summary.trace_export(name=\"my_func_trace\", step=0, profiler_outdir=logdir)\n","\n","tf.summary.trace_off()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOmLBxGdRzsA"},"outputs":[],"source":["data_generator = DataGenerator(dataset_path='')\n","\n","print(data_generator)"]},{"cell_type":"code","source":["number_of_batches = len(data_generator)\n","print(\"Number of batches:\", number_of_batches)"],"metadata":{"id":"u_fkI8FGHkU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gT_IJCt7RzsE"},"outputs":[],"source":["a, p, n = data_generator[4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sICebsXDRzsI"},"outputs":[],"source":["%tensorboard --logdir /content/drive/My\\ Drive/\n","# %tensorboard --logdir ./logs/\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4sylyqShRzsL"},"outputs":[],"source":["checkpoint = tf.train.Checkpoint(model=model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64QIpJ5bRzsS"},"outputs":[],"source":["losses = []\n","accuracy = []\n","\n","no_of_batches = data_generator.__len__()\n","for i in range(1, epochs+1, 1):\n","    loss = 0\n","    with tqdm(total=no_of_batches) as pbar:\n","\n","        description = \"Epoch \" + str(i) + \"/\" + str(epochs)\n","        pbar.set_description_str(description)\n","\n","        for j in range(no_of_batches):\n","            try:\n","\n","              data = data_generator[j]\n","              temp = train(data)\n","              loss += temp\n","\n","              pbar.update()\n","              print_statement = \"Loss :\" + str(temp.numpy())\n","              pbar.set_postfix_str(print_statement)\n","\n","            except Exception as e:\n","                print(f\"Error in batch {j}: {e}\")\n","                break  # Break the loop on error for debugging\n","\n","        loss /= no_of_batches\n","        losses.append(loss.numpy())\n","        with file_writer.as_default():\n","            tf.summary.scalar('Loss', data=loss.numpy(), step=i)\n","\n","        print_statement = \"Loss :\" + str(loss.numpy())\n","\n","        pbar.set_postfix_str(print_statement)\n","\n","checkpoint.save(checkpoint_path)\n","print(\"Checkpoint Saved\")"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}