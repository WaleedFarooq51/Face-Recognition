{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bsb52TO23NT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/My Drive/"],"metadata":{"id":"O-AoS0h3343K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pe2jedQHwl11"},"outputs":[],"source":["import os\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import dlib\n","import pickle\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","import tensorflow as tf\n","from datetime import datetime\n","from imutils import face_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8cvEmClwl2C"},"outputs":[],"source":["base_dir = \"/content/drive/My Drive/\"\n","checkpoint_path = os.path.join(base_dir, 'logs/model/siamese-1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v21JvvtJwl2E"},"outputs":[],"source":["'''\n","The following code cell is taken from the source code of keras_vggface.'\n","I tried using the preprocess_input function provided by tf.keras but they provide different results.\n","To my knowledge, it seems that the mean values which are subtracted in each image are different.\n","'''\n","K = tf.keras.backend\n","\n","def preprocess_input(x, data_format=None, version=1):\n","    x_temp = np.copy(x)\n","    if data_format is None:\n","        data_format = K.image_data_format()\n","    assert data_format in {'channels_last', 'channels_first'}\n","\n","    if version == 1:\n","        if data_format == 'channels_first':\n","            x_temp = x_temp[:, ::-1, ...]\n","            x_temp[:, 0, :, :] -= 93.5940\n","            x_temp[:, 1, :, :] -= 104.7624\n","            x_temp[:, 2, :, :] -= 129.1863\n","        else:\n","            x_temp = x_temp[..., ::-1]\n","            x_temp[..., 0] -= 93.5940\n","            x_temp[..., 1] -= 104.7624\n","            x_temp[..., 2] -= 129.1863\n","\n","    elif version == 2:\n","        if data_format == 'channels_first':\n","            x_temp = x_temp[:, ::-1, ...]\n","            x_temp[:, 0, :, :] -= 91.4953\n","            x_temp[:, 1, :, :] -= 103.8827\n","            x_temp[:, 2, :, :] -= 131.0912\n","        else:\n","            x_temp = x_temp[..., ::-1]\n","            x_temp[..., 0] -= 91.4953\n","            x_temp[..., 1] -= 103.8827\n","            x_temp[..., 2] -= 131.0912\n","    else:\n","        raise NotImplementedError\n","\n","    return x_temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJX-AgCzwl2K"},"outputs":[],"source":["vggface = tf.keras.models.Sequential()\n","vggface.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding=\"SAME\", input_shape=(224,224, 3)))\n","vggface.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n","vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n","\n","vggface.add(tf.keras.layers.Flatten())\n","\n","vggface.add(tf.keras.layers.Dense(4096, activation='relu'))\n","vggface.add(tf.keras.layers.Dropout(0.5))\n","vggface.add(tf.keras.layers.Dense(4096, activation='relu'))\n","vggface.add(tf.keras.layers.Dropout(0.5))\n","vggface.add(tf.keras.layers.Dense(2622, activation='softmax'))\n","\n","vggface.pop()\n","vggface.add(tf.keras.layers.Dense(128, use_bias=False))\n","\n","for layer in vggface.layers[:-2]:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SE_ytpXwl2N"},"outputs":[],"source":["class SiameseNetwork(tf.keras.Model):\n","    def __init__(self, vgg_face):\n","        super(SiameseNetwork, self).__init__()\n","        self.vgg_face = vgg_face\n","\n","    @tf.function\n","    def call(self, inputs):\n","        image_1, image_2, image_3 =  inputs\n","        with tf.name_scope(\"Anchor\") as scope:\n","            feature_1 = self.vgg_face(image_1)\n","            feature_1 = tf.math.l2_normalize(feature_1, axis=-1)\n","        with tf.name_scope(\"Positive\") as scope:\n","            feature_2 = self.vgg_face(image_2)\n","            feature_2 = tf.math.l2_normalize(feature_2, axis=-1)\n","        with tf.name_scope(\"Negative\") as scope:\n","            feature_3 = self.vgg_face(image_3)\n","            feature_3 = tf.math.l2_normalize(feature_3, axis=-1)\n","        return [feature_1, feature_2, feature_3]\n","\n","    @tf.function\n","    def get_features(self, inputs):\n","        return tf.math.l2_normalize(self.vgg_face(inputs), axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0jlPucKwl2O"},"outputs":[],"source":["model = SiameseNetwork(vggface)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPZYCMaDwl2Q"},"outputs":[],"source":["_ = model([tf.zeros((32,224,224,3)), tf.zeros((32,224,224,3)), tf.zeros((32,224,224,3))])\n","_ = model.get_features(tf.zeros((32,224,224,3)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0UzDUYCwl2R"},"outputs":[],"source":["checkpoint = tf.train.Checkpoint(model=model)\n","checkpoint.restore(checkpoint_path)"]},{"cell_type":"markdown","metadata":{"id":"SMQQAyM3wl2U"},"source":["# Gallery images data collection details\n","The cell below should be re-run for data collection for multiple people as a gallery images.\n","For a single person, you could  collect 8-10 images tops. Entering the name of the person would add a directory to the data folder which will be the name of that person.\n","If the name of the person exists it will give an error. So make sure to keep different names."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"-V1S331Zwl2T","executionInfo":{"status":"ok","timestamp":1756716356529,"user_tz":-300,"elapsed":8,"user":{"displayName":"Muhammad Khan","userId":"08467596017353836838"}}},"outputs":[],"source":["## Add path to you test Gallery images\n","data_dir = '/test_Gallery_data/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSNRi-Euwl2e"},"outputs":[],"source":["name = input(\"Enter the name of the person : \")\n","os.mkdir(os.path.join(data_dir, name))\n","cap = cv2.VideoCapture(0)\n","if not cap.isOpened():\n","    print(\"Error: Could not open camera.\")\n","count = 0\n","while True:\n","    ret, frame = cap.read()\n","    cv2_imshow(frame)\n","    k = cv2.waitKey(1)\n","    if k == ord('s'):\n","        cv2.imwrite(os.path.join(data_dir, name + '/' + str(count) + '.png') , frame)\n","        count += 1\n","    if k ==ord('q'):\n","        break\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sILXskwKwl2f"},"outputs":[],"source":["people = sorted(os.listdir(data_dir))\n","print(people)\n","face_detector = dlib.get_frontal_face_detector()\n","features = []\n","dumpable_features = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSxX1elJwl2g"},"outputs":[],"source":["for person in people:\n","    person_path = os.path.join(data_dir, person)\n","    print(person_path)\n","    images = []\n","    for image in os.listdir(person_path):\n","        image_path = os.path.join(person_path, image)\n","        img = cv2.imread(image_path)\n","        #print(image_path)\n","        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","        faces = face_detector(gray, 0)\n","        if len(faces) == 1:\n","            for face in faces:\n","                face_bounding_box = face_utils.rect_to_bb(face)\n","                if all(i >= 0 for i in face_bounding_box):\n","                    [x, y, w, h] = face_bounding_box\n","                    frame = img[y:y + h, x:x + w]\n","                    frame = cv2.resize(frame, (224, 224))\n","                    frame = np.asarray(frame, dtype=np.float64)\n","                    images.append(frame)\n","    images = np.asarray(images)\n","    images = preprocess_input(images)\n","    images = tf.convert_to_tensor(images)\n","    feature = model.get_features(images)\n","    feature = tf.reduce_mean(feature, axis=0)\n","    features.append(feature.numpy())\n","    dumpable_features[person] = feature.numpy()\n"]},{"cell_type":"code","source":["for i, feature in enumerate(features):\n","    person_name = people[i]\n","    print(f\"Features for {person_name}: {feature}\")"],"metadata":{"id":"2481VsqwPqAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjq7FIxCwl2h"},"outputs":[],"source":["features = np.asarray(features)\n","print(features)\n","print(features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PqD2FWsAwl2h"},"outputs":[],"source":["cap = cv2.VideoCapture(0)\n","count = 0\n","name = 'not identified'\n","while True:\n","    ret, img = cap.read()\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","    faces = face_detector(gray, 0)\n","    for face in faces:\n","        face_bounding_box = face_utils.rect_to_bb(face)\n","        if all(i >= 0 for i in face_bounding_box):\n","            [x, y, w, h] = face_bounding_box\n","            frame = img[y:y + h, x:x + w]\n","            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","            frame = cv2.resize(frame, (224, 224))\n","            frame = np.asarray(frame, dtype=np.float64)\n","            frame = np.expand_dims(frame, axis=0)\n","            frame = preprocess_input(frame)\n","            feature = model.get_features(frame)\n","\n","            dist = tf.norm(features - feature, axis=1)\n","            name = 'not identified'\n","            loc = tf.argmin(dist)\n","            if dist[loc] < 0.8:\n","                name = people[loc]\n","            else:\n","#                     print(dist.numpy())\n","                pass\n","\n","            font_face = cv2.FONT_HERSHEY_SIMPLEX\n","            cv2.putText(img, name, (x, y-5), font_face, 0.8, (0,0,255), 3)\n","    cv2.imshow('Image', img)\n","    k = cv2.waitKey(1)\n","    if k ==ord('q'):\n","        break\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","source":["## Add path to your test Probe images\n","test_img = '/content/drive/My Drive/'"],"metadata":{"id":"l56nQGj4MNox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["name = 'not identified'\n","img = cv2.imread(test_img)\n","gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","faces = face_detector(gray, 0)\n","\n","for face in faces:\n","        face_bounding_box = face_utils.rect_to_bb(face)\n","        if all(i >= 0 for i in face_bounding_box):\n","            [x, y, w, h] = face_bounding_box\n","            frame = img[y:y + h, x:x + w]\n","            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","            frame = cv2.resize(frame, (224, 224))\n","            frame = np.asarray(frame, dtype=np.float64)\n","            frame = np.expand_dims(frame, axis=0)\n","            frame = preprocess_input(frame)\n","            feature = model.get_features(frame)\n","\n","            dist = tf.norm(features - feature, axis=1)\n","            print(dist)\n","            loc = tf.argmin(dist)\n","            print(loc)\n","\n","            if dist[loc] < 0.8:\n","                name = people[loc]\n","                print(\"IDENTIFIED\")\n","            else:\n","                     print(\"NOT IDENTIFIED\")\n","                     pass\n","\n","            font_face = cv2.FONT_HERSHEY_SIMPLEX\n","            cv2.putText(img, name, (x, y-5), font_face, 0.8, (0,0,255), 3)\n","\n","cv2_imshow(img)"],"metadata":{"id":"hpZaLkFwEiWJ"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}